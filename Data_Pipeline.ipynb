{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Data Pipeline Architecture\n",
        "In this code, I present you the data pipeline architecture in the below flowchart."
      ],
      "metadata": {
        "id": "jNlMTAg4Eirm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "def show_pipeline_flowchart():\n",
        "    dot = Digraph(comment=\"Universal Data Pipeline\", format=\"png\")\n",
        "    dot.attr(rankdir=\"TB\", splines=\"ortho\", nodesep=\"0.6\", ranksep=\"0.8\")\n",
        "    dot.attr(\"node\", shape=\"box\", style=\"rounded,filled\", fontsize=\"10\", fontname=\"Helvetica\", fillcolor=\"#E8F4FF\")\n",
        "\n",
        "    # ETL\n",
        "    dot.node(\"ETL\", \"ETL\\n(Extract → Transform → Load)\", fillcolor=\"#CCE5FF\")\n",
        "    dot.node(\"Extract\", \"extract_data()\")\n",
        "    dot.node(\"Transform\", \"transform_data()\")\n",
        "    dot.node(\"Load\", \"load_data()\")\n",
        "\n",
        "    # Analysis\n",
        "    dot.node(\"Analysis\", \"exploratory_analysis()\", fillcolor=\"#FFE0B2\")\n",
        "    dot.node(\"FindCorr\", \"find_high_correlations()\")\n",
        "\n",
        "    # Visualization\n",
        "    dot.node(\"Viz\", \"create_visualizations()\", fillcolor=\"#C8E6C9\")\n",
        "    dot.node(\"Overview\", \"create_overview_dashboard()\")\n",
        "    dot.node(\"Dist\", \"create_distribution_plots()\")\n",
        "    dot.node(\"Heatmap\", \"create_correlation_heatmap()\")\n",
        "    dot.node(\"Missing\", \"create_missing_values_plot()\")\n",
        "    dot.node(\"FeatImp\", \"create_feature_importance_plot()\")\n",
        "\n",
        "    # Feature Engineering\n",
        "    dot.node(\"FE\", \"feature_engineering()\", fillcolor=\"#E1BEE7\")\n",
        "    dot.node(\"Target\", \"auto_detect_target()\")\n",
        "    dot.node(\"Encode\", \"encode_categorical_features()\")\n",
        "    dot.node(\"Scale\", \"scale_numerical_features()\")\n",
        "    dot.node(\"Derived\", \"create_derived_features()\")\n",
        "    dot.node(\"RemoveCorr\", \"remove_correlated_features()\")\n",
        "    dot.node(\"Outliers\", \"handle_outliers()\")\n",
        "\n",
        "    # Report\n",
        "    dot.node(\"Report\", \"generate_final_report()\", fillcolor=\"#FFCDD2\")\n",
        "\n",
        "    # Edges\n",
        "    dot.edges([(\"ETL\", \"Extract\"), (\"Extract\", \"Transform\"), (\"Transform\", \"Load\")])\n",
        "    dot.edge(\"Load\", \"Analysis\")\n",
        "    dot.edge(\"Analysis\", \"FindCorr\")\n",
        "\n",
        "    dot.edge(\"Load\", \"Viz\")\n",
        "    dot.edges([\n",
        "        (\"Viz\", \"Overview\"), (\"Viz\", \"Dist\"), (\"Viz\", \"Heatmap\"),\n",
        "        (\"Viz\", \"Missing\"), (\"Viz\", \"FeatImp\")\n",
        "    ])\n",
        "\n",
        "    dot.edge(\"Load\", \"FE\")\n",
        "    dot.edges([\n",
        "        (\"FE\", \"Target\"), (\"Target\", \"Encode\"), (\"Encode\", \"Scale\"),\n",
        "        (\"Scale\", \"Derived\"), (\"Derived\", \"RemoveCorr\"), (\"RemoveCorr\", \"Outliers\")\n",
        "    ])\n",
        "\n",
        "    # Report connections\n",
        "    dot.edge(\"FE\", \"Report\")\n",
        "    dot.edge(\"Analysis\", \"Report\")\n",
        "    dot.edge(\"Viz\", \"Report\")\n",
        "    dot.edge(\"Load\", \"Report\")\n",
        "\n",
        "    # Render and display\n",
        "    return dot\n",
        "\n",
        "# Generate and display flowchart\n",
        "flowchart = show_pipeline_flowchart()\n",
        "flowchart.render(\"pipeline_flowchart\", view=True)  # saves & opens PNG\n",
        "flowchart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bCc-rOZLEiRF",
        "outputId": "6d20a2f0-5411-40f7-a7bd-a69b6aeed3e0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"1383pt\" height=\"984pt\"\n viewBox=\"0.00 0.00 1383.00 984.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 980)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-980 1379,-980 1379,4 -4,4\"/>\n<!-- ETL -->\n<g id=\"node1\" class=\"node\">\n<title>ETL</title>\n<path fill=\"#cce5ff\" stroke=\"black\" d=\"M526.5,-976C526.5,-976 400.5,-976 400.5,-976 394.5,-976 388.5,-970 388.5,-964 388.5,-964 388.5,-952 388.5,-952 388.5,-946 394.5,-940 400.5,-940 400.5,-940 526.5,-940 526.5,-940 532.5,-940 538.5,-946 538.5,-952 538.5,-952 538.5,-964 538.5,-964 538.5,-970 532.5,-976 526.5,-976\"/>\n<text text-anchor=\"middle\" x=\"463.5\" y=\"-961\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\">ETL</text>\n<text text-anchor=\"middle\" x=\"463.5\" y=\"-950\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\">(Extract → Transform → Load)</text>\n</g>\n<!-- Extract -->\n<g id=\"node2\" class=\"node\">\n<title>Extract</title>\n<path fill=\"#e8f4ff\" stroke=\"black\" d=\"M490,-882C490,-882 437,-882 437,-882 431,-882 425,-876 425,-870 425,-870 425,-858 425,-858 425,-852 431,-846 437,-846 437,-846 490,-846 490,-846 496,-846 502,-852 502,-858 502,-858 502,-870 502,-870 502,-876 496,-882 490,-882\"/>\n<text text-anchor=\"middle\" x=\"463.5\" y=\"-861.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\">extract_data()</text>\n</g>\n<!-- ETL&#45;&gt;Extract -->\n<g id=\"edge1\" class=\"edge\">\n<title>ETL&#45;&gt;Extract</title>\n<path fill=\"none\" stroke=\"black\" d=\"M463.5,-939.88C463.5,-939.88 463.5,-892.07 463.5,-892.07\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"467,-892.07 463.5,-882.07 460,-892.07 467,-892.07\"/>\n</g>\n<!-- Transform -->\n<g id=\"node3\" class=\"node\">\n<title>Transform</title>\n<path fill=\"#e8f4ff\" stroke=\"black\" d=\"M495.5,-788C495.5,-788 431.5,-788 431.5,-788 425.5,-788 419.5,-782 419.5,-776 419.5,-776 419.5,-764 419.5,-764 419.5,-758 425.5,-752 431.5,-752 431.5,-752 495.5,-752 495.5,-752 501.5,-752 507.5,-758 507.5,-764 507.5,-764 507.5,-776 507.5,-776 507.5,-782 501.5,-788 495.5,-788\"/>\n<text text-anchor=\"middle\" x=\"463.5\" y=\"-767.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\">transform_data()</text>\n</g>\n<!-- Extract&#45;&gt;Transform -->\n<g id=\"edge2\" class=\"edge\">\n<title>Extract&#45;&gt;Transform</title>\n<path fill=\"none\" stroke=\"black\" d=\"M463.5,-845.88C463.5,-845.88 463.5,-798.07 463.5,-798.07\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"467,-798.07 463.5,-788.07 460,-798.07 467,-798.07\"/>\n</g>\n<!-- Load -->\n<g id=\"node4\" class=\"node\">\n<title>Load</title>\n<path fill=\"#e8f4ff\" stroke=\"black\" d=\"M484,-694C484,-694 443,-694 443,-694 437,-694 431,-688 431,-682 431,-682 431,-670 431,-670 431,-664 437,-658 443,-658 443,-658 484,-658 484,-658 490,-658 496,-664 496,-670 496,-670 496,-682 496,-682 496,-688 490,-694 484,-694\"/>\n<text text-anchor=\"middle\" x=\"463.5\" y=\"-673.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\">load_data()</text>\n</g>\n<!-- Transform&#45;&gt;Load -->\n<g id=\"edge3\" class=\"edge\">\n<title>Transform&#45;&gt;Load</title>\n<path fill=\"none\" stroke=\"black\" d=\"M463.5,-751.88C463.5,-751.88 463.5,-704.07 463.5,-704.07\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"467,-704.07 463.5,-694.07 460,-704.07 467,-704.07\"/>\n</g>\n<!-- Analysis -->\n<g id=\"node5\" class=\"node\">\n<title>Analysis</title>\n<path fill=\"#ffe0b2\" stroke=\"black\" d=\"M260.5,-600C260.5,-600 172.5,-600 172.5,-600 166.5,-600 160.5,-594 160.5,-588 160.5,-588 160.5,-576 160.5,-576 160.5,-570 166.5,-564 172.5,-564 172.5,-564 260.5,-564 260.5,-564 266.5,-564 272.5,-570 272.5,-576 272.5,-576 272.5,-588 272.5,-588 272.5,-594 266.5,-600 260.5,-600\"/>\n<text text-anchor=\"middle\" x=\"216.5\" y=\"-579.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\">exploratory_analysis()</text>\n</g>\n<!-- Load&#45;&gt;Analysis -->\n<g id=\"edge4\" class=\"edge\">\n<title>Load&#45;&gt;Analysis</title>\n<path fill=\"none\" stroke=\"black\" d=\"M453,-657.77C453,-631.99 453,-588 453,-588 453,-588 282.55,-588 282.55,-588\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"282.55,-584.5 272.55,-588 282.55,-591.5 282.55,-584.5\"/>\n</g>\n<!-- Viz -->\n<g id=\"node7\" class=\"node\">\n<title>Viz</title>\n<path fill=\"#c8e6c9\" stroke=\"black\" d=\"M602.5,-600C602.5,-600 512.5,-600 512.5,-600 506.5,-600 500.5,-594 500.5,-588 500.5,-588 500.5,-576 500.5,-576 500.5,-570 506.5,-564 512.5,-564 512.5,-564 602.5,-564 602.5,-564 608.5,-564 614.5,-570 614.5,-576 614.5,-576 614.5,-588 614.5,-588 614.5,-594 608.5,-600 602.5,-600\"/>\n<text text-anchor=\"middle\" x=\"557.5\" y=\"-579.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\">create_visualizations()</text>\n</g>\n<!-- Load&#45;&gt;Viz -->\n<g id=\"edge6\" class=\"edge\">\n<title>Load&#45;&gt;Viz</title>\n<path fill=\"none\" stroke=\"black\" d=\"M496.19,-670C500.8,-670 504,-670 504,-670 504,-670 504,-610.23 504,-610.23\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"507.5,-610.23 504,-600.23 500.5,-610.23 507.5,-610.23\"/>\n</g>\n<!-- FE -->\n<g id=\"node13\" class=\"node\">\n<title>FE</title>\n<path fill=\"#e1bee7\" stroke=\"black\" d=\"M753,-600C753,-600 670,-600 670,-600 664,-600 658,-594 658,-588 658,-588 658,-576 658,-576 658,-570 664,-564 670,-564 670,-564 753,-564 753,-564 759,-564 765,-570 765,-576 765,-576 765,-588 765,-588 765,-594 759,-600 753,-600\"/>\n<text text-anchor=\"middle\" x=\"711.5\" y=\"-579.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\">feature_engineering()</text>\n</g>\n<!-- Load&#45;&gt;FE -->\n<g id=\"edge12\" class=\"edge\">\n<title>Load&#45;&gt;FE</title>\n<path fill=\"none\" stroke=\"black\" d=\"M496.18,-682C563.52,-682 711.5,-682 711.5,-682 711.5,-682 711.5,-610.34 711.5,-610.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"715,-610.34 711.5,-600.34 708,-610.34 715,-610.34\"/>\n</g>\n<!-- Report -->\n<g id=\"node20\" class=\"node\">\n<title>Report</title>\n<path fill=\"#ffcdd2\" stroke=\"black\" d=\"M261,-506C261,-506 172,-506 172,-506 166,-506 160,-500 160,-494 160,-494 160,-482 160,-482 160,-476 166,-470 172,-470 172,-470 261,-470 261,-470 267,-470 273,-476 273,-482 273,-482 273,-494 273,-494 273,-500 267,-506 261,-506\"/>\n<text text-anchor=\"middle\" x=\"216.5\" y=\"-485.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\">generate_final_report()</text>\n</g>\n<!-- Load&#45;&gt;Report -->\n<g id=\"edge22\" class=\"edge\">\n<title>Load&#45;&gt;Report</title>\n<path fill=\"none\" stroke=\"black\" d=\"M430.78,-676C379.83,-676 287.33,-676 287.33,-676 287.33,-676 287.33,-494 287.33,-494 287.33,-494 283.34,-494 283.34,-494\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"283.34,-490.5 273.34,-494 283.34,-497.5 283.34,-490.5\"/>\n</g>\n<!-- FindCorr -->\n<g id=\"node6\" class=\"node\">\n<title>FindCorr</title>\n<path fill=\"#e8f4ff\" stroke=\"black\" d=\"M105,-506C105,-506 12,-506 12,-506 6,-506 0,-500 0,-494 0,-494 0,-482 0,-482 0,-476 6,-470 12,-470 12,-470 105,-470 105,-470 111,-470 117,-476 117,-482 117,-482 117,-494 117,-494 117,-500 111,-506 105,-506\"/>\n<text text-anchor=\"middle\" x=\"58.5\" y=\"-485.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\">find_high_correlations()</text>\n</g>\n<!-- Analysis&#45;&gt;FindCorr -->\n<g id=\"edge5\" class=\"edge\">\n<title>Analysis&#45;&gt;FindCorr</title>\n<path fill=\"none\" stroke=\"black\" d=\"M160.35,-582C114.94,-582 58.5,-582 58.5,-582 58.5,-582 58.5,-516.12 58.5,-516.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"62,-516.12 58.5,-506.12 55,-516.12 62,-516.12\"/>\n</g>\n<!-- Analysis&#45;&gt;Report -->\n<g id=\"edge20\" class=\"edge\">\n<title>Analysis&#45;&gt;Report</title>\n<path fill=\"none\" stroke=\"black\" d=\"M197.83,-563.88C197.83,-563.88 197.83,-516.07 197.83,-516.07\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"201.33,-516.07 197.83,-506.07 194.33,-516.07 201.33,-516.07\"/>\n</g>\n<!-- Overview -->\n<g id=\"node8\" class=\"node\">\n<title>Overview</title>\n<path fill=\"#e8f4ff\" stroke=\"black\" d=\"M650.5,-506C650.5,-506 530.5,-506 530.5,-506 524.5,-506 518.5,-500 518.5,-494 518.5,-494 518.5,-482 518.5,-482 518.5,-476 524.5,-470 530.5,-470 530.5,-470 650.5,-470 650.5,-470 656.5,-470 662.5,-476 662.5,-482 662.5,-482 662.5,-494 662.5,-494 662.5,-500 656.5,-506 650.5,-506\"/>\n<text text-anchor=\"middle\" x=\"590.5\" y=\"-485.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\">create_overview_dashboard()</text>\n</g>\n<!-- Viz&#45;&gt;Overview -->\n<g id=\"edge7\" class=\"edge\">\n<title>Viz&#45;&gt;Overview</title>\n<path fill=\"none\" stroke=\"black\" d=\"M537.7,-563.88C537.7,-563.88 537.7,-516.07 537.7,-516.07\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"541.2,-516.07 537.7,-506.07 534.2,-516.07 541.2,-516.07\"/>\n</g>\n<!-- Dist -->\n<g id=\"node9\" class=\"node\">\n<title>Dist</title>\n<path fill=\"#e8f4ff\" stroke=\"black\" d=\"M821.5,-506C821.5,-506 717.5,-506 717.5,-506 711.5,-506 705.5,-500 705.5,-494 705.5,-494 705.5,-482 705.5,-482 705.5,-476 711.5,-470 717.5,-470 717.5,-470 821.5,-470 821.5,-470 827.5,-470 833.5,-476 833.5,-482 833.5,-482 833.5,-494 833.5,-494 833.5,-500 827.5,-506 821.5,-506\"/>\n<text text-anchor=\"middle\" x=\"769.5\" y=\"-485.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\">create_distribution_plots()</text>\n</g>\n<!-- Viz&#45;&gt;Dist -->\n<g id=\"edge8\" class=\"edge\">\n<title>Viz&#45;&gt;Dist</title>\n<path fill=\"none\" stroke=\"black\" d=\"M556.9,-563.89C556.9,-544.66 556.9,-517 556.9,-517 556.9,-517 735.25,-517 735.25,-517 735.25,-517 735.25,-515.92 735.25,-515.92\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"738.75,-516.2 735.25,-506.2 731.75,-516.2 738.75,-516.2\"/>\n</g>\n<!-- Heatmap -->\n<g id=\"node10\" class=\"node\">\n<title>Heatmap</title>\n<path fill=\"#e8f4ff\" stroke=\"black\" d=\"M1008,-506C1008,-506 889,-506 889,-506 883,-506 877,-500 877,-494 877,-494 877,-482 877,-482 877,-476 883,-470 889,-470 889,-470 1008,-470 1008,-470 1014,-470 1020,-476 1020,-482 1020,-482 1020,-494 1020,-494 1020,-500 1014,-506 1008,-506\"/>\n<text text-anchor=\"middle\" x=\"948.5\" y=\"-485.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\">create_correlation_heatmap()</text>\n</g>\n<!-- Viz&#45;&gt;Heatmap -->\n<g id=\"edge9\" class=\"edge\">\n<title>Viz&#45;&gt;Heatmap</title>\n<path fill=\"none\" stroke=\"black\" d=\"M576.1,-563.76C576.1,-548.43 576.1,-529 576.1,-529 576.1,-529 948.5,-529 948.5,-529 948.5,-529 948.5,-516.1 948.5,-516.1\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"952,-516.1 948.5,-506.1 945,-516.1 952,-516.1\"/>\n</g>\n<!-- Missing -->\n<g id=\"node11\" class=\"node\">\n<title>Missing</title>\n<path fill=\"#e8f4ff\" stroke=\"black\" d=\"M1194,-506C1194,-506 1075,-506 1075,-506 1069,-506 1063,-500 1063,-494 1063,-494 1063,-482 1063,-482 1063,-476 1069,-470 1075,-470 1075,-470 1194,-470 1194,-470 1200,-470 1206,-476 1206,-482 1206,-482 1206,-494 1206,-494 1206,-500 1200,-506 1194,-506\"/>\n<text text-anchor=\"middle\" x=\"1134.5\" y=\"-485.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\">create_missing_values_plot()</text>\n</g>\n<!-- Viz&#45;&gt;Missing -->\n<g id=\"edge10\" class=\"edge\">\n<title>Viz&#45;&gt;Missing</title>\n<path fill=\"none\" stroke=\"black\" d=\"M595.3,-563.86C595.3,-557.54 595.3,-552 595.3,-552 595.3,-552 1134.5,-552 1134.5,-552 1134.5,-552 1134.5,-516.17 1134.5,-516.17\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1138,-516.17 1134.5,-506.17 1131,-516.17 1138,-516.17\"/>\n</g>\n<!-- FeatImp -->\n<g id=\"node12\" class=\"node\">\n<title>FeatImp</title>\n<path fill=\"#e8f4ff\" stroke=\"black\" d=\"M463,-506C463,-506 328,-506 328,-506 322,-506 316,-500 316,-494 316,-494 316,-482 316,-482 316,-476 322,-470 328,-470 328,-470 463,-470 463,-470 469,-470 475,-476 475,-482 475,-482 475,-494 475,-494 475,-500 469,-506 463,-506\"/>\n<text text-anchor=\"middle\" x=\"395.5\" y=\"-485.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\">create_feature_importance_plot()</text>\n</g>\n<!-- Viz&#45;&gt;FeatImp -->\n<g id=\"edge11\" class=\"edge\">\n<title>Viz&#45;&gt;FeatImp</title>\n<path fill=\"none\" stroke=\"black\" d=\"M509.5,-563.88C509.5,-536.56 509.5,-488 509.5,-488 509.5,-488 485.3,-488 485.3,-488\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"485.3,-484.5 475.3,-488 485.3,-491.5 485.3,-484.5\"/>\n</g>\n<!-- Viz&#45;&gt;Report -->\n<g id=\"edge21\" class=\"edge\">\n<title>Viz&#45;&gt;Report</title>\n<path fill=\"none\" stroke=\"black\" d=\"M500.4,-576C425,-576 301.67,-576 301.67,-576 301.67,-576 301.67,-482 301.67,-482 301.67,-482 283.31,-482 283.31,-482\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"283.31,-478.5 273.31,-482 283.31,-485.5 283.31,-478.5\"/>\n</g>\n<!-- Target -->\n<g id=\"node14\" class=\"node\">\n<title>Target</title>\n<path fill=\"#e8f4ff\" stroke=\"black\" d=\"M1341.5,-506C1341.5,-506 1261.5,-506 1261.5,-506 1255.5,-506 1249.5,-500 1249.5,-494 1249.5,-494 1249.5,-482 1249.5,-482 1249.5,-476 1255.5,-470 1261.5,-470 1261.5,-470 1341.5,-470 1341.5,-470 1347.5,-470 1353.5,-476 1353.5,-482 1353.5,-482 1353.5,-494 1353.5,-494 1353.5,-500 1347.5,-506 1341.5,-506\"/>\n<text text-anchor=\"middle\" x=\"1301.5\" y=\"-485.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\">auto_detect_target()</text>\n</g>\n<!-- FE&#45;&gt;Target -->\n<g id=\"edge13\" class=\"edge\">\n<title>FE&#45;&gt;Target</title>\n<path fill=\"none\" stroke=\"black\" d=\"M765.16,-582C910.6,-582 1301.5,-582 1301.5,-582 1301.5,-582 1301.5,-516.12 1301.5,-516.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1305,-516.12 1301.5,-506.12 1298,-516.12 1305,-516.12\"/>\n</g>\n<!-- FE&#45;&gt;Report -->\n<g id=\"edge19\" class=\"edge\">\n<title>FE&#45;&gt;Report</title>\n<path fill=\"none\" stroke=\"black\" d=\"M684,-563.94C684,-552.56 684,-540 684,-540 684,-540 235.17,-540 235.17,-540 235.17,-540 235.17,-516.19 235.17,-516.19\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"238.67,-516.19 235.17,-506.19 231.67,-516.19 238.67,-516.19\"/>\n</g>\n<!-- Encode -->\n<g id=\"node15\" class=\"node\">\n<title>Encode</title>\n<path fill=\"#e8f4ff\" stroke=\"black\" d=\"M1363,-412C1363,-412 1240,-412 1240,-412 1234,-412 1228,-406 1228,-400 1228,-400 1228,-388 1228,-388 1228,-382 1234,-376 1240,-376 1240,-376 1363,-376 1363,-376 1369,-376 1375,-382 1375,-388 1375,-388 1375,-400 1375,-400 1375,-406 1369,-412 1363,-412\"/>\n<text text-anchor=\"middle\" x=\"1301.5\" y=\"-391.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\">encode_categorical_features()</text>\n</g>\n<!-- Target&#45;&gt;Encode -->\n<g id=\"edge14\" class=\"edge\">\n<title>Target&#45;&gt;Encode</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1301.5,-469.88C1301.5,-469.88 1301.5,-422.07 1301.5,-422.07\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1305,-422.07 1301.5,-412.07 1298,-422.07 1305,-422.07\"/>\n</g>\n<!-- Scale -->\n<g id=\"node16\" class=\"node\">\n<title>Scale</title>\n<path fill=\"#e8f4ff\" stroke=\"black\" d=\"M1356.5,-318C1356.5,-318 1246.5,-318 1246.5,-318 1240.5,-318 1234.5,-312 1234.5,-306 1234.5,-306 1234.5,-294 1234.5,-294 1234.5,-288 1240.5,-282 1246.5,-282 1246.5,-282 1356.5,-282 1356.5,-282 1362.5,-282 1368.5,-288 1368.5,-294 1368.5,-294 1368.5,-306 1368.5,-306 1368.5,-312 1362.5,-318 1356.5,-318\"/>\n<text text-anchor=\"middle\" x=\"1301.5\" y=\"-297.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\">scale_numerical_features()</text>\n</g>\n<!-- Encode&#45;&gt;Scale -->\n<g id=\"edge15\" class=\"edge\">\n<title>Encode&#45;&gt;Scale</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1301.5,-375.88C1301.5,-375.88 1301.5,-328.07 1301.5,-328.07\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1305,-328.07 1301.5,-318.07 1298,-328.07 1305,-328.07\"/>\n</g>\n<!-- Derived -->\n<g id=\"node17\" class=\"node\">\n<title>Derived</title>\n<path fill=\"#e8f4ff\" stroke=\"black\" d=\"M1353,-224C1353,-224 1250,-224 1250,-224 1244,-224 1238,-218 1238,-212 1238,-212 1238,-200 1238,-200 1238,-194 1244,-188 1250,-188 1250,-188 1353,-188 1353,-188 1359,-188 1365,-194 1365,-200 1365,-200 1365,-212 1365,-212 1365,-218 1359,-224 1353,-224\"/>\n<text text-anchor=\"middle\" x=\"1301.5\" y=\"-203.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\">create_derived_features()</text>\n</g>\n<!-- Scale&#45;&gt;Derived -->\n<g id=\"edge16\" class=\"edge\">\n<title>Scale&#45;&gt;Derived</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1301.5,-281.88C1301.5,-281.88 1301.5,-234.07 1301.5,-234.07\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1305,-234.07 1301.5,-224.07 1298,-234.07 1305,-234.07\"/>\n</g>\n<!-- RemoveCorr -->\n<g id=\"node18\" class=\"node\">\n<title>RemoveCorr</title>\n<path fill=\"#e8f4ff\" stroke=\"black\" d=\"M1361,-130C1361,-130 1242,-130 1242,-130 1236,-130 1230,-124 1230,-118 1230,-118 1230,-106 1230,-106 1230,-100 1236,-94 1242,-94 1242,-94 1361,-94 1361,-94 1367,-94 1373,-100 1373,-106 1373,-106 1373,-118 1373,-118 1373,-124 1367,-130 1361,-130\"/>\n<text text-anchor=\"middle\" x=\"1301.5\" y=\"-109.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\">remove_correlated_features()</text>\n</g>\n<!-- Derived&#45;&gt;RemoveCorr -->\n<g id=\"edge17\" class=\"edge\">\n<title>Derived&#45;&gt;RemoveCorr</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1301.5,-187.88C1301.5,-187.88 1301.5,-140.07 1301.5,-140.07\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1305,-140.07 1301.5,-130.07 1298,-140.07 1305,-140.07\"/>\n</g>\n<!-- Outliers -->\n<g id=\"node19\" class=\"node\">\n<title>Outliers</title>\n<path fill=\"#e8f4ff\" stroke=\"black\" d=\"M1333.5,-36C1333.5,-36 1269.5,-36 1269.5,-36 1263.5,-36 1257.5,-30 1257.5,-24 1257.5,-24 1257.5,-12 1257.5,-12 1257.5,-6 1263.5,0 1269.5,0 1269.5,0 1333.5,0 1333.5,0 1339.5,0 1345.5,-6 1345.5,-12 1345.5,-12 1345.5,-24 1345.5,-24 1345.5,-30 1339.5,-36 1333.5,-36\"/>\n<text text-anchor=\"middle\" x=\"1301.5\" y=\"-15.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\">handle_outliers()</text>\n</g>\n<!-- RemoveCorr&#45;&gt;Outliers -->\n<g id=\"edge18\" class=\"edge\">\n<title>RemoveCorr&#45;&gt;Outliers</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1301.5,-93.88C1301.5,-93.88 1301.5,-46.07 1301.5,-46.07\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1305,-46.07 1301.5,-36.07 1298,-46.07 1305,-46.07\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7fd49352f9b0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Universal Data Pipeline Architecture\n",
        "# A complete ETL, Analysis, Visualization & Feature Engineering Blueprint\n",
        "# Author: Data Pipeline Template\n",
        "# Usage: Simply replace 'your_data.csv' with your dataset and run\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, mean_squared_error\n",
        "import warnings\n",
        "import os\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class UniversalDataPipeline:\n",
        "    def __init__(self, config_file=\"pipeline_config.json\"):\n",
        "        \"\"\"\n",
        "        Universal Data Pipeline for ETL, Analysis, Visualization & Feature Engineering\n",
        "        \"\"\"\n",
        "        self.config = self.load_config(config_file)\n",
        "        self.raw_data = None\n",
        "        self.cleaned_data = None\n",
        "        self.processed_data = None\n",
        "        self.features = None\n",
        "        self.target = None\n",
        "        self.report = {}\n",
        "\n",
        "    def load_config(self, config_file):\n",
        "        \"\"\"Load pipeline configuration\"\"\"\n",
        "        default_config = {\n",
        "            \"data_file\": \"your_data.csv\",  # CHANGE THIS TO YOUR DATA FILE\n",
        "            \"target_column\": None,  # Will auto-detect if None\n",
        "            \"missing_threshold\": 0.5,  # Drop columns with >50% missing values\n",
        "            \"correlation_threshold\": 0.95,  # Remove highly correlated features\n",
        "            \"test_size\": 0.2,\n",
        "            \"random_state\": 42,\n",
        "            \"output_dir\": \"pipeline_output\"\n",
        "        }\n",
        "\n",
        "        if os.path.exists(config_file):\n",
        "            with open(config_file, 'r') as f:\n",
        "                config = json.load(f)\n",
        "            # Merge with defaults\n",
        "            for key, value in default_config.items():\n",
        "                if key not in config:\n",
        "                    config[key] = value\n",
        "        else:\n",
        "            config = default_config\n",
        "            # Save default config\n",
        "            with open(config_file, 'w') as f:\n",
        "                json.dump(config, f, indent=4)\n",
        "\n",
        "        return config\n",
        "\n",
        "    def setup_output_directory(self):\n",
        "        \"\"\"Create output directory structure\"\"\"\n",
        "        dirs = [\n",
        "            self.config['output_dir'],\n",
        "            f\"{self.config['output_dir']}/visualizations\",\n",
        "            f\"{self.config['output_dir']}/processed_data\",\n",
        "            f\"{self.config['output_dir']}/reports\"\n",
        "        ]\n",
        "        for dir_path in dirs:\n",
        "            os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "    # ================== ETL COMPONENTS ==================\n",
        "\n",
        "    def extract_data(self):\n",
        "        \"\"\"Extract data from various sources\"\"\"\n",
        "        print(\"🔄 EXTRACTING DATA...\")\n",
        "\n",
        "        file_path = self.config['data_file']\n",
        "\n",
        "        try:\n",
        "            # Auto-detect file type and load\n",
        "            if file_path.endswith('.csv'):\n",
        "                self.raw_data = pd.read_csv(file_path)\n",
        "            elif file_path.endswith(('.xlsx', '.xls')):\n",
        "                self.raw_data = pd.read_excel(file_path)\n",
        "            elif file_path.endswith('.json'):\n",
        "                self.raw_data = pd.read_json(file_path)\n",
        "            elif file_path.endswith('.parquet'):\n",
        "                self.raw_data = pd.read_parquet(file_path)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "            print(f\"✅ Data extracted successfully: {self.raw_data.shape}\")\n",
        "            self.report['extraction'] = {\n",
        "                'status': 'success',\n",
        "                'shape': self.raw_data.shape,\n",
        "                'file_type': file_path.split('.')[-1]\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Data extraction failed: {str(e)}\")\n",
        "            self.report['extraction'] = {'status': 'failed', 'error': str(e)}\n",
        "            raise\n",
        "\n",
        "    def transform_data(self):\n",
        "        \"\"\"Transform and clean the data\"\"\"\n",
        "        print(\"\\n🔧 TRANSFORMING DATA...\")\n",
        "\n",
        "        self.cleaned_data = self.raw_data.copy()\n",
        "\n",
        "        # 1. Basic info and statistics\n",
        "        print(f\"Original shape: {self.cleaned_data.shape}\")\n",
        "\n",
        "        # 2. Handle missing values\n",
        "        missing_cols = self.cleaned_data.isnull().sum()\n",
        "        missing_cols = missing_cols[missing_cols > 0]\n",
        "\n",
        "        if not missing_cols.empty:\n",
        "            print(f\"Columns with missing values: {len(missing_cols)}\")\n",
        "\n",
        "            # Drop columns with too many missing values\n",
        "            threshold = self.config['missing_threshold']\n",
        "            cols_to_drop = missing_cols[missing_cols / len(self.cleaned_data) > threshold].index\n",
        "            if len(cols_to_drop) > 0:\n",
        "                self.cleaned_data.drop(columns=cols_to_drop, inplace=True)\n",
        "                print(f\"Dropped {len(cols_to_drop)} columns with >{threshold*100}% missing values\")\n",
        "\n",
        "            # Fill remaining missing values\n",
        "            for col in self.cleaned_data.columns:\n",
        "                if self.cleaned_data[col].isnull().any():\n",
        "                    if self.cleaned_data[col].dtype in ['object', 'category']:\n",
        "                        # Categorical: fill with mode\n",
        "                        mode_val = self.cleaned_data[col].mode().iloc[0] if not self.cleaned_data[col].mode().empty else 'Unknown'\n",
        "                        self.cleaned_data[col].fillna(mode_val, inplace=True)\n",
        "                    else:\n",
        "                        # Numerical: fill with median\n",
        "                        median_val = self.cleaned_data[col].median()\n",
        "                        self.cleaned_data[col].fillna(median_val, inplace=True)\n",
        "\n",
        "        # 3. Remove duplicates\n",
        "        initial_shape = self.cleaned_data.shape\n",
        "        self.cleaned_data.drop_duplicates(inplace=True)\n",
        "        if initial_shape[0] != self.cleaned_data.shape[0]:\n",
        "            print(f\"Removed {initial_shape[0] - self.cleaned_data.shape[0]} duplicate rows\")\n",
        "\n",
        "        # 4. Data type optimization\n",
        "        self.optimize_datatypes()\n",
        "\n",
        "        print(f\"✅ Data transformed: {self.cleaned_data.shape}\")\n",
        "\n",
        "        self.report['transformation'] = {\n",
        "            'original_shape': self.raw_data.shape,\n",
        "            'cleaned_shape': self.cleaned_data.shape,\n",
        "            'missing_columns_handled': len(missing_cols),\n",
        "            'duplicates_removed': initial_shape[0] - self.cleaned_data.shape[0]\n",
        "        }\n",
        "\n",
        "    def optimize_datatypes(self):\n",
        "        \"\"\"Optimize data types to reduce memory usage\"\"\"\n",
        "        for col in self.cleaned_data.columns:\n",
        "            if self.cleaned_data[col].dtype == 'object':\n",
        "                # Try to convert to category if unique values < 50% of total\n",
        "                unique_count = self.cleaned_data[col].nunique()\n",
        "                total_count = len(self.cleaned_data[col])\n",
        "                if unique_count / total_count < 0.5:\n",
        "                    self.cleaned_data[col] = self.cleaned_data[col].astype('category')\n",
        "            elif self.cleaned_data[col].dtype in ['int64', 'int32']:\n",
        "                # Downcast integers\n",
        "                self.cleaned_data[col] = pd.to_numeric(self.cleaned_data[col], downcast='integer')\n",
        "            elif self.cleaned_data[col].dtype in ['float64', 'float32']:\n",
        "                # Downcast floats\n",
        "                self.cleaned_data[col] = pd.to_numeric(self.cleaned_data[col], downcast='float')\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Save processed data\"\"\"\n",
        "        print(\"\\n💾 LOADING PROCESSED DATA...\")\n",
        "\n",
        "        # Save cleaned data\n",
        "        output_path = f\"{self.config['output_dir']}/processed_data/cleaned_data.csv\"\n",
        "        self.cleaned_data.to_csv(output_path, index=False)\n",
        "        print(f\"✅ Cleaned data saved to: {output_path}\")\n",
        "\n",
        "        self.report['loading'] = {\n",
        "            'cleaned_data_path': output_path,\n",
        "            'status': 'success'\n",
        "        }\n",
        "\n",
        "    # ================== ANALYSIS COMPONENTS ==================\n",
        "\n",
        "    def exploratory_analysis(self):\n",
        "        \"\"\"Comprehensive exploratory data analysis\"\"\"\n",
        "        print(\"\\n📊 PERFORMING EXPLORATORY ANALYSIS...\")\n",
        "\n",
        "        analysis_report = {}\n",
        "\n",
        "        # 1. Basic statistics\n",
        "        analysis_report['basic_stats'] = {\n",
        "            'shape': self.cleaned_data.shape,\n",
        "            'memory_usage_mb': self.cleaned_data.memory_usage(deep=True).sum() / (1024**2),\n",
        "            'numeric_columns': len(self.cleaned_data.select_dtypes(include=[np.number]).columns),\n",
        "            'categorical_columns': len(self.cleaned_data.select_dtypes(include=['object', 'category']).columns)\n",
        "        }\n",
        "\n",
        "        # 2. Column analysis\n",
        "        column_info = []\n",
        "        for col in self.cleaned_data.columns:\n",
        "            info = {\n",
        "                'column': col,\n",
        "                'dtype': str(self.cleaned_data[col].dtype),\n",
        "                'unique_values': self.cleaned_data[col].nunique(),\n",
        "                'missing_count': self.cleaned_data[col].isnull().sum(),\n",
        "                'missing_percentage': (self.cleaned_data[col].isnull().sum() / len(self.cleaned_data)) * 100\n",
        "            }\n",
        "\n",
        "            if self.cleaned_data[col].dtype in [np.number]:\n",
        "                info.update({\n",
        "                    'mean': self.cleaned_data[col].mean(),\n",
        "                    'std': self.cleaned_data[col].std(),\n",
        "                    'min': self.cleaned_data[col].min(),\n",
        "                    'max': self.cleaned_data[col].max(),\n",
        "                    'skewness': self.cleaned_data[col].skew()\n",
        "                })\n",
        "\n",
        "            column_info.append(info)\n",
        "\n",
        "        analysis_df = pd.DataFrame(column_info)\n",
        "        analysis_report['column_analysis'] = column_info\n",
        "\n",
        "        # Save analysis report\n",
        "        analysis_df.to_csv(f\"{self.config['output_dir']}/reports/column_analysis.csv\", index=False)\n",
        "\n",
        "        # 3. Correlation analysis for numeric columns\n",
        "        numeric_cols = self.cleaned_data.select_dtypes(include=[np.number]).columns\n",
        "        if len(numeric_cols) > 1:\n",
        "            correlation_matrix = self.cleaned_data[numeric_cols].corr()\n",
        "            analysis_report['high_correlations'] = self.find_high_correlations(correlation_matrix)\n",
        "\n",
        "        self.report['analysis'] = analysis_report\n",
        "        print(\"✅ Exploratory analysis completed\")\n",
        "\n",
        "    def find_high_correlations(self, corr_matrix, threshold=None):\n",
        "        \"\"\"Find highly correlated feature pairs\"\"\"\n",
        "        if threshold is None:\n",
        "            threshold = self.config['correlation_threshold']\n",
        "\n",
        "        high_corr_pairs = []\n",
        "        for i in range(len(corr_matrix.columns)):\n",
        "            for j in range(i+1, len(corr_matrix.columns)):\n",
        "                corr_val = abs(corr_matrix.iloc[i, j])\n",
        "                if corr_val > threshold:\n",
        "                    high_corr_pairs.append({\n",
        "                        'feature1': corr_matrix.columns[i],\n",
        "                        'feature2': corr_matrix.columns[j],\n",
        "                        'correlation': corr_val\n",
        "                    })\n",
        "\n",
        "        return high_corr_pairs\n",
        "\n",
        "    # ================== VISUALIZATION COMPONENTS ==================\n",
        "\n",
        "    def create_visualizations(self):\n",
        "        \"\"\"Generate comprehensive visualizations\"\"\"\n",
        "        print(\"\\n📈 CREATING VISUALIZATIONS...\")\n",
        "\n",
        "        plt.style.use('default')\n",
        "\n",
        "        # 1. Data Overview Dashboard\n",
        "        self.create_overview_dashboard()\n",
        "\n",
        "        # 2. Distribution plots for numeric columns\n",
        "        self.create_distribution_plots()\n",
        "\n",
        "        # 3. Correlation heatmap\n",
        "        self.create_correlation_heatmap()\n",
        "\n",
        "        # 4. Missing values visualization\n",
        "        self.create_missing_values_plot()\n",
        "\n",
        "        # 5. Feature importance (if target is available)\n",
        "        if self.target is not None:\n",
        "            self.create_feature_importance_plot()\n",
        "\n",
        "        print(\"✅ Visualizations created\")\n",
        "\n",
        "    def create_overview_dashboard(self):\n",
        "        \"\"\"Create a comprehensive overview dashboard\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('Data Overview Dashboard', fontsize=16, fontweight='bold')\n",
        "\n",
        "        # Data types distribution\n",
        "        dtype_counts = self.cleaned_data.dtypes.value_counts()\n",
        "        axes[0, 0].pie(dtype_counts.values, labels=dtype_counts.index, autopct='%1.1f%%')\n",
        "        axes[0, 0].set_title('Data Types Distribution')\n",
        "\n",
        "        # Missing values by column\n",
        "        missing_data = self.cleaned_data.isnull().sum().sort_values(ascending=False)[:10]\n",
        "        if missing_data.sum() > 0:\n",
        "            axes[0, 1].bar(range(len(missing_data)), missing_data.values)\n",
        "            axes[0, 1].set_xticks(range(len(missing_data)))\n",
        "            axes[0, 1].set_xticklabels(missing_data.index, rotation=45)\n",
        "            axes[0, 1].set_title('Top 10 Columns with Missing Values')\n",
        "        else:\n",
        "            axes[0, 1].text(0.5, 0.5, 'No Missing Values', ha='center', va='center')\n",
        "            axes[0, 1].set_title('Missing Values')\n",
        "\n",
        "        # Unique values per column (top 10)\n",
        "        numeric_cols = self.cleaned_data.select_dtypes(include=[np.number]).columns[:10]\n",
        "        if len(numeric_cols) > 0:\n",
        "            unique_counts = [self.cleaned_data[col].nunique() for col in numeric_cols]\n",
        "            axes[1, 0].bar(range(len(numeric_cols)), unique_counts)\n",
        "            axes[1, 0].set_xticks(range(len(numeric_cols)))\n",
        "            axes[1, 0].set_xticklabels(numeric_cols, rotation=45)\n",
        "            axes[1, 0].set_title('Unique Values Count (Numeric Cols)')\n",
        "\n",
        "        # Memory usage by column (top 10)\n",
        "        memory_usage = self.cleaned_data.memory_usage(deep=True).sort_values(ascending=False)[:10]\n",
        "        axes[1, 1].bar(range(len(memory_usage)), memory_usage.values)\n",
        "        axes[1, 1].set_xticks(range(len(memory_usage)))\n",
        "        axes[1, 1].set_xticklabels(memory_usage.index, rotation=45)\n",
        "        axes[1, 1].set_title('Memory Usage by Column (Bytes)')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.config['output_dir']}/visualizations/overview_dashboard.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def create_distribution_plots(self):\n",
        "        \"\"\"Create distribution plots for numeric columns\"\"\"\n",
        "        numeric_cols = self.cleaned_data.select_dtypes(include=[np.number]).columns\n",
        "        if len(numeric_cols) == 0:\n",
        "            return\n",
        "\n",
        "        n_cols = min(4, len(numeric_cols))\n",
        "        n_rows = (len(numeric_cols) - 1) // n_cols + 1\n",
        "\n",
        "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
        "        if n_rows == 1:\n",
        "            axes = axes.reshape(1, -1) if n_cols > 1 else [axes]\n",
        "\n",
        "        for idx, col in enumerate(numeric_cols):\n",
        "            row = idx // n_cols\n",
        "            col_idx = idx % n_cols\n",
        "\n",
        "            ax = axes[row][col_idx] if n_cols > 1 else axes[row]\n",
        "\n",
        "            # Histogram with KDE\n",
        "            self.cleaned_data[col].hist(bins=30, alpha=0.7, ax=ax)\n",
        "            ax.axvline(self.cleaned_data[col].mean(), color='red', linestyle='--', label='Mean')\n",
        "            ax.axvline(self.cleaned_data[col].median(), color='green', linestyle='--', label='Median')\n",
        "            ax.set_title(f'Distribution of {col}')\n",
        "            ax.legend()\n",
        "\n",
        "        # Hide empty subplots\n",
        "        for idx in range(len(numeric_cols), n_rows * n_cols):\n",
        "            row = idx // n_cols\n",
        "            col_idx = idx % n_cols\n",
        "            axes[row][col_idx].set_visible(False)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.config['output_dir']}/visualizations/distributions.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def create_correlation_heatmap(self):\n",
        "        \"\"\"Create correlation heatmap for numeric columns\"\"\"\n",
        "        numeric_cols = self.cleaned_data.select_dtypes(include=[np.number]).columns\n",
        "        if len(numeric_cols) < 2:\n",
        "            return\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        correlation_matrix = self.cleaned_data[numeric_cols].corr()\n",
        "\n",
        "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm',\n",
        "                   center=0, square=True, linewidths=0.5)\n",
        "        plt.title('Correlation Heatmap of Numeric Features')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.config['output_dir']}/visualizations/correlation_heatmap.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def create_missing_values_plot(self):\n",
        "        \"\"\"Visualize missing values pattern\"\"\"\n",
        "        missing_data = self.cleaned_data.isnull().sum()\n",
        "        missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
        "\n",
        "        if len(missing_data) == 0:\n",
        "            return\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.bar(range(len(missing_data)), missing_data.values)\n",
        "        plt.xticks(range(len(missing_data)), missing_data.index, rotation=45)\n",
        "        plt.title('Missing Values by Column')\n",
        "        plt.ylabel('Number of Missing Values')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.config['output_dir']}/visualizations/missing_values.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    def create_feature_importance_plot(self):\n",
        "        \"\"\"Create feature importance plot using Random Forest\"\"\"\n",
        "        if self.target is None or len(self.features.columns) == 0:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # Prepare data\n",
        "            X = self.features.select_dtypes(include=[np.number])\n",
        "            y = self.target\n",
        "\n",
        "            if len(X.columns) < 2:\n",
        "                return\n",
        "\n",
        "            # Determine if classification or regression\n",
        "            if y.dtype == 'object' or y.nunique() < 10:\n",
        "                model = RandomForestClassifier(n_estimators=100, random_state=self.config['random_state'])\n",
        "            else:\n",
        "                model = RandomForestRegressor(n_estimators=100, random_state=self.config['random_state'])\n",
        "\n",
        "            model.fit(X, y)\n",
        "\n",
        "            # Get feature importance\n",
        "            importance_df = pd.DataFrame({\n",
        "                'feature': X.columns,\n",
        "                'importance': model.feature_importances_\n",
        "            }).sort_values('importance', ascending=False).head(20)\n",
        "\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            plt.barh(range(len(importance_df)), importance_df['importance'])\n",
        "            plt.yticks(range(len(importance_df)), importance_df['feature'])\n",
        "            plt.xlabel('Feature Importance')\n",
        "            plt.title('Top 20 Feature Importances (Random Forest)')\n",
        "            plt.gca().invert_yaxis()\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"{self.config['output_dir']}/visualizations/feature_importance.png\", dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Could not create feature importance plot: {str(e)}\")\n",
        "\n",
        "    # ================== FEATURE ENGINEERING COMPONENTS ==================\n",
        "\n",
        "    def feature_engineering(self):\n",
        "        \"\"\"Comprehensive feature engineering\"\"\"\n",
        "        print(\"\\n⚙️  PERFORMING FEATURE ENGINEERING...\")\n",
        "\n",
        "        self.processed_data = self.cleaned_data.copy()\n",
        "\n",
        "        # 1. Auto-detect target column if not specified\n",
        "        if self.config['target_column'] is None:\n",
        "            self.config['target_column'] = self.auto_detect_target()\n",
        "\n",
        "        if self.config['target_column'] and self.config['target_column'] in self.processed_data.columns:\n",
        "            self.target = self.processed_data[self.config['target_column']]\n",
        "            self.features = self.processed_data.drop(columns=[self.config['target_column']])\n",
        "        else:\n",
        "            self.target = None\n",
        "            self.features = self.processed_data.copy()\n",
        "\n",
        "        # 2. Encode categorical variables\n",
        "        self.encode_categorical_features()\n",
        "\n",
        "        # 3. Scale numerical features\n",
        "        self.scale_numerical_features()\n",
        "\n",
        "        # 4. Create new features\n",
        "        self.create_derived_features()\n",
        "\n",
        "        # 5. Remove highly correlated features\n",
        "        self.remove_correlated_features()\n",
        "\n",
        "        # 6. Handle outliers\n",
        "        self.handle_outliers()\n",
        "\n",
        "        print(f\"✅ Feature engineering completed: {self.features.shape}\")\n",
        "\n",
        "        # Save processed features\n",
        "        self.features.to_csv(f\"{self.config['output_dir']}/processed_data/engineered_features.csv\", index=False)\n",
        "        if self.target is not None:\n",
        "            self.target.to_csv(f\"{self.config['output_dir']}/processed_data/target.csv\", index=False)\n",
        "\n",
        "        self.report['feature_engineering'] = {\n",
        "            'original_features': len(self.cleaned_data.columns),\n",
        "            'final_features': len(self.features.columns),\n",
        "            'target_column': self.config['target_column']\n",
        "        }\n",
        "\n",
        "    def auto_detect_target(self):\n",
        "        \"\"\"Auto-detect potential target column\"\"\"\n",
        "        # Common target column names\n",
        "        target_keywords = ['target', 'label', 'class', 'outcome', 'result', 'prediction', 'y', 'price', 'value']\n",
        "\n",
        "        for col in self.processed_data.columns:\n",
        "            if any(keyword in col.lower() for keyword in target_keywords):\n",
        "                print(f\"🎯 Auto-detected target column: {col}\")\n",
        "                return col\n",
        "\n",
        "        # If no obvious target, use the last column\n",
        "        if len(self.processed_data.columns) > 1:\n",
        "            last_col = self.processed_data.columns[-1]\n",
        "            print(f\"🎯 Using last column as target: {last_col}\")\n",
        "            return last_col\n",
        "\n",
        "        print(\"⚠️  No target column detected\")\n",
        "        return None\n",
        "\n",
        "    def encode_categorical_features(self):\n",
        "        \"\"\"Encode categorical variables\"\"\"\n",
        "        categorical_cols = self.features.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "        if len(categorical_cols) == 0:\n",
        "            return\n",
        "\n",
        "        print(f\"Encoding {len(categorical_cols)} categorical columns...\")\n",
        "\n",
        "        for col in categorical_cols:\n",
        "            unique_values = self.features[col].nunique()\n",
        "\n",
        "            if unique_values == 2:\n",
        "                # Binary encoding\n",
        "                le = LabelEncoder()\n",
        "                self.features[col] = le.fit_transform(self.features[col].astype(str))\n",
        "            elif unique_values <= 10:\n",
        "                # One-hot encoding for low cardinality\n",
        "                dummies = pd.get_dummies(self.features[col], prefix=col, dummy_na=False)\n",
        "                self.features = pd.concat([self.features.drop(columns=[col]), dummies], axis=1)\n",
        "            else:\n",
        "                # Label encoding for high cardinality\n",
        "                le = LabelEncoder()\n",
        "                self.features[col] = le.fit_transform(self.features[col].astype(str))\n",
        "\n",
        "    def scale_numerical_features(self):\n",
        "        \"\"\"Scale numerical features\"\"\"\n",
        "        numeric_cols = self.features.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "        if len(numeric_cols) == 0:\n",
        "            return\n",
        "\n",
        "        print(f\"Scaling {len(numeric_cols)} numerical columns...\")\n",
        "\n",
        "        # Use StandardScaler\n",
        "        scaler = StandardScaler()\n",
        "        self.features[numeric_cols] = scaler.fit_transform(self.features[numeric_cols])\n",
        "\n",
        "    def create_derived_features(self):\n",
        "        \"\"\"Create derived features from existing ones\"\"\"\n",
        "        numeric_cols = self.features.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "        if len(numeric_cols) < 2:\n",
        "            return\n",
        "\n",
        "        print(\"Creating derived features...\")\n",
        "\n",
        "        # Take only first few columns to avoid explosion\n",
        "        base_cols = numeric_cols[:5]\n",
        "\n",
        "        for i, col1 in enumerate(base_cols):\n",
        "            for col2 in base_cols[i+1:]:\n",
        "                # Interaction features (product)\n",
        "                self.features[f'{col1}_x_{col2}'] = self.features[col1] * self.features[col2]\n",
        "\n",
        "                # Ratio features (avoid division by zero)\n",
        "                denominator = self.features[col2].replace(0, np.nan)\n",
        "                self.features[f'{col1}_div_{col2}'] = self.features[col1] / denominator\n",
        "                self.features[f'{col1}_div_{col2}'].fillna(0, inplace=True)\n",
        "\n",
        "        # Statistical features for each numeric column\n",
        "        for col in base_cols:\n",
        "            # Squared features\n",
        "            self.features[f'{col}_squared'] = self.features[col] ** 2\n",
        "\n",
        "            # Log features (handle negative values)\n",
        "            if self.features[col].min() > 0:\n",
        "                self.features[f'{col}_log'] = np.log1p(self.features[col])\n",
        "\n",
        "    def remove_correlated_features(self):\n",
        "        \"\"\"Remove highly correlated features\"\"\"\n",
        "        numeric_cols = self.features.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "        if len(numeric_cols) < 2:\n",
        "            return\n",
        "\n",
        "        # Calculate correlation matrix\n",
        "        corr_matrix = self.features[numeric_cols].corr().abs()\n",
        "\n",
        "        # Find highly correlated pairs\n",
        "        upper_triangle = corr_matrix.where(\n",
        "            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
        "        )\n",
        "\n",
        "        # Find features to remove\n",
        "        to_remove = [column for column in upper_triangle.columns\n",
        "                    if any(upper_triangle[column] > self.config['correlation_threshold'])]\n",
        "\n",
        "        if to_remove:\n",
        "            print(f\"Removing {len(to_remove)} highly correlated features...\")\n",
        "            self.features.drop(columns=to_remove, inplace=True)\n",
        "\n",
        "    def handle_outliers(self):\n",
        "        \"\"\"Handle outliers using IQR method\"\"\"\n",
        "        numeric_cols = self.features.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "        if len(numeric_cols) == 0:\n",
        "            return\n",
        "\n",
        "        print(\"Handling outliers...\")\n",
        "\n",
        "        for col in numeric_cols:\n",
        "            Q1 = self.features[col].quantile(0.25)\n",
        "            Q3 = self.features[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "\n",
        "            # Define outlier bounds\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "            # Cap outliers instead of removing them\n",
        "            self.features[col] = self.features[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "    # ================== MAIN PIPELINE RUNNER ==================\n",
        "\n",
        "    def run_pipeline(self):\n",
        "        \"\"\"Run the complete pipeline\"\"\"\n",
        "        print(\"🚀 STARTING UNIVERSAL DATA PIPELINE\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        # Setup\n",
        "        self.setup_output_directory()\n",
        "\n",
        "        # ETL Process\n",
        "        self.extract_data()\n",
        "        self.transform_data()\n",
        "        self.load_data()\n",
        "\n",
        "        # Analysis\n",
        "        self.exploratory_analysis()\n",
        "\n",
        "        # Visualization\n",
        "        self.create_visualizations()\n",
        "\n",
        "        # Feature Engineering\n",
        "        self.feature_engineering()\n",
        "\n",
        "        # Generate final report\n",
        "        self.generate_final_report(start_time)\n",
        "\n",
        "        print(\"\\n🎉 PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "        print(f\"📁 Check '{self.config['output_dir']}' directory for all outputs\")\n",
        "\n",
        "    def generate_final_report(self, start_time):\n",
        "        \"\"\"Generate comprehensive pipeline report\"\"\"\n",
        "        end_time = datetime.now()\n",
        "        duration = end_time - start_time\n",
        "\n",
        "        self.report['pipeline_info'] = {\n",
        "            'start_time': start_time.isoformat(),\n",
        "            'end_time': end_time.isoformat(),\n",
        "            'duration_seconds': duration.total_seconds(),\n",
        "            'config_used': self.config\n",
        "        }\n",
        "\n",
        "        # Save report as JSON\n",
        "        with open(f\"{self.config['output_dir']}/reports/pipeline_report.json\", 'w') as f:\n",
        "            json.dump(self.report, f, indent=4, default=str)\n",
        "\n",
        "        # Create summary report\n",
        "        summary = f\"\"\"\n",
        "# Data Pipeline Summary Report\n",
        "\n",
        "## Pipeline Execution\n",
        "- **Start Time**: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\n",
        "- **End Time**: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\n",
        "- **Duration**: {duration.total_seconds():.2f} seconds\n",
        "\n",
        "## Data Overview\n",
        "- **Original Shape**: {self.report.get('transformation', {}).get('original_shape', 'N/A')}\n",
        "- **Final Shape**: {self.report.get('transformation', {}).get('cleaned_shape', 'N/A')}\n",
        "- **Features After Engineering**: {self.report.get('feature_engineering', {}).get('final_features', 'N/A')}\n",
        "- **Target Column**: {self.report.get('feature_engineering', {}).get('target_column', 'None detected')}\n",
        "\n",
        "## Processing Statistics\n",
        "- **Missing Columns Handled**: {self.report.get('transformation', {}).get('missing_columns_handled', 0)}\n",
        "- **Duplicates Removed**: {self.report.get('transformation', {}).get('duplicates_removed', 0)}\n",
        "- **Memory Usage**: {self.report.get('analysis', {}).get('basic_stats', {}).get('memory_usage_mb', 0):.2f} MB\n",
        "\n",
        "## Files Generated\n",
        "- 📊 Visualizations: `visualizations/` directory\n",
        "- 📈 Processed Data: `processed_data/` directory\n",
        "- 📋 Reports: `reports/`"
      ],
      "metadata": {
        "id": "ArPIDWecELLR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}